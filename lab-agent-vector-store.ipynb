{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efad6479-7fb1-40c4-8610-8963f8de307f",
   "metadata": {},
   "source": [
    "# Lab | Agent & Vector store\n",
    "\n",
    "**Change the state union dataset and replicate this lab by updating the prompts accordingly.**\n",
    "\n",
    "One such dataset is the [sonnets.txt](https://github.com/martin-gorner/tensorflow-rnn-shakespeare/blob/master/shakespeare/sonnets.txt) dataset or any other data of your choice from the same git."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b24990",
   "metadata": {},
   "source": [
    "# Combine agents and vector stores\n",
    "\n",
    "This notebook covers how to combine agents and vector stores. The use case for this is that you've ingested your data into a vector store and want to interact with it in an agentic manner.\n",
    "\n",
    "The recommended method for doing so is to create a `RetrievalQA` and then use that as a tool in the overall agent. Let's take a look at doing this below. You can do this with multiple different vector DBs, and use the agent as a way to route between them. There are two different ways of doing this - you can either let the agent use the vector stores as normal tools, or you can set `return_direct=True` to really just use the agent as a router."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b22020a",
   "metadata": {},
   "source": [
    "## Create the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeff3a2d-b0cc-429f-a34c-4381d71f1a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Ensure the key exists\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57afaec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader, WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14331eec-fd46-42e0-b6e7-adaf21824ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM + embeddings\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b7b772b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download sonnets.txt\n",
    "url = \"https://raw.githubusercontent.com/martin-gorner/tensorflow-rnn-shakespeare/master/shakespeare/sonnets.txt\"\n",
    "local_path = \"sonnets.txt\"\n",
    "r = requests.get(url, timeout=30)\n",
    "r.raise_for_status()\n",
    "with open(local_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(r.text)\n",
    "\n",
    "# Load text file\n",
    "loader = TextLoader(local_path, encoding=\"utf-8\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2675861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Vector store + retriever\n",
    "sonnets_db = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"sonnets\"\n",
    ")\n",
    "sonnets_retriever = sonnets_db.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc5403d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The themes about time in these sonnets include the inevitability of aging and decay, the transient nature of beauty, and the struggle against time's destructive force. Time is depicted as a relentless force that causes beauty and life to fade, as seen in the imagery of night overtaking day, flowers past their prime, and barren trees. The sonnets reflect on how everything is subject to time's scythe, which ultimately takes away loved ones and beauty. However, there is also a sense of hope, as the speaker suggests that through poetry, love and beauty can be preserved and celebrated despite time's cruel hand.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LCEL chain\n",
    "sonnets_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You answer questions using ONLY the provided context from Shakespeare's sonnets. \"\n",
    "               \"If the answer isn't in the context, say you don't know.\"),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "sonnets_chain = (\n",
    "    {\n",
    "        \"context\": sonnets_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | sonnets_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "sonnets_chain.invoke(\"What themes about time appear in these sonnets?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dbe8923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ruff FAQ webpage\n",
    "web_loader = WebBaseLoader(\n",
    "    web_paths=[\"https://beta.ruff.rs/docs/faq/\"]\n",
    ")\n",
    "ruff_docs = web_loader.load()\n",
    "\n",
    "# Split using same splitter\n",
    "ruff_splits = text_splitter.split_documents(ruff_docs)\n",
    "\n",
    "# Vector store + retriever\n",
    "ruff_db = Chroma.from_documents(\n",
    "    documents=ruff_splits,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"ruff\"\n",
    ")\n",
    "ruff_retriever = ruff_db.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92e28522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, Ruff supports pyproject.toml configuration, but if you prefer not to use it, you can use a ruff.toml file for configuration instead. The two files are functionally equivalent.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LCEL chain\n",
    "ruff_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You answer questions using ONLY the provided context from the Ruff FAQ. \"\n",
    "               \"If the answer isn't in the context, say you don't know.\"),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
    "])\n",
    "\n",
    "ruff_chain = (\n",
    "    {\n",
    "        \"context\": ruff_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | ruff_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "ruff_chain.invoke(\"Does Ruff support pyproject.toml configuration?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a6c031",
   "metadata": {},
   "source": [
    "## Create the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb142786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import things that are needed generically\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96090b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "@tool\n",
    "def sonnets_qa(question: str) -> str:\n",
    "    \"\"\"Answer questions about Shakespeare's sonnets.\"\"\"\n",
    "    return sonnets_chain.invoke(question)\n",
    "\n",
    "@tool\n",
    "def ruff_qa(question: str) -> str:\n",
    "    \"\"\"Answer questions about Ruff, the Python linter.\"\"\"\n",
    "    return ruff_chain.invoke(question)\n",
    "\n",
    "agent = create_agent(model,tools=[sonnets_qa, ruff_qa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7167955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_text(result):\n",
    "    \"\"\"Extract the final assistant text from a LangGraph-style agent result.\"\"\"\n",
    "    # Most commonly result is a dict with 'messages'\n",
    "    if isinstance(result, dict) and \"messages\" in result:\n",
    "        msgs = result[\"messages\"]\n",
    "        # walk backwards to find the last AIMessage with content\n",
    "        for m in reversed(msgs):\n",
    "            if isinstance(m, AIMessage) and getattr(m, \"content\", None):\n",
    "                return m.content\n",
    "        # fallback: last message content if it exists\n",
    "        if msgs and hasattr(msgs[-1], \"content\"):\n",
    "            return msgs[-1].content\n",
    "\n",
    "    # If it's already a string (some chains), just return it\n",
    "    if isinstance(result, str):\n",
    "        return result\n",
    "\n",
    "    # Last resort\n",
    "    return str(result)\n",
    "\n",
    "def ask(agent, q: str) -> str:\n",
    "    res = agent.invoke({\"messages\": [HumanMessage(q)]})\n",
    "    return final_text(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f30071a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In his State of the Union address, President Joe Biden praised Ketanji Brown Jackson, highlighting her historic confirmation as the first Black woman to serve on the U.S. Supreme Court. He emphasized her qualifications, experience, and the importance of her appointment in representing diversity on the Court. Biden's remarks were part of a broader discussion on the significance of judicial appointments and the need for a judiciary that reflects the nation's diversity.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(agent, \"What did biden say about ketanji brown jackson in the state of the union address?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "388e4c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ruff can be used as a drop-in replacement for Flake8, especially when used without or with a small number of plugins, and is designed to work well alongside Black on Python 3 code. Here are some key reasons to consider using Ruff over Flake8:\\n\\n1. **Different Rule Set**: Ruff implements a different set of rules and uses different rule codes and prefixes than Flake8 plugins, which helps minimize conflicts.\\n\\n2. **Comprehensive Coverage**: Ruff implements over 800 rules, making it a more comprehensive tool compared to Flake8's total rules.\\n\\n3. **Performance**: Ruff is designed to be faster than Flake8, which can be beneficial for larger codebases.\\n\\n4. **Simplicity**: It can serve as a simpler solution when you don't need custom lint rules or the full set of opinionated rules from Flake8 plugins like flake8-bugbear.\\n\\nHowever, it's worth noting that Ruff does not support custom lint rules and may not include all the specific rules that some Flake8 plugins provide.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(agent, \"Why use Ruff over Flake8?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a9b5e",
   "metadata": {},
   "source": [
    "## Use the Agent solely as a router"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9161ba91",
   "metadata": {},
   "source": [
    "You can also set `return_direct=True` if you intend to use the agent as a router and just want to directly return the result of the RetrievalQAChain.\n",
    "\n",
    "Notice that in the above examples the agent did some extra work after querying the RetrievalQAChain. You can avoid that and just return the result directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f59b377e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "router_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "@tool\n",
    "def sonnets_qa(question: str) -> str:\n",
    "    \"\"\"Answer questions about Shakespeare's sonnets.\"\"\"\n",
    "    return sonnets_chain.invoke(question)\n",
    "\n",
    "@tool\n",
    "def ruff_qa(question: str) -> str:\n",
    "    \"\"\"Answer questions about Ruff, the Python linter.\"\"\"\n",
    "    return ruff_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8615707a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create agent (router behavior comes from instruction + tool outputs)\n",
    "router_agent = create_agent(router_model, tools=[sonnets_qa, ruff_qa],)\n",
    "\n",
    "def route(q: str):\n",
    "    # force router behavior in-message\n",
    "    msg = (\n",
    "        \"You are a router. Choose the single best tool and call it once. \"\n",
    "        \"Return EXACTLY the tool output with no extra commentary.\\n\\n\"\n",
    "        f\"Question: {q}\"\n",
    "    )\n",
    "    res = router_agent.invoke({\"messages\": [HumanMessage(msg)]})\n",
    "    return res[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edfd0a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ruff can be used as a drop-in replacement for Flake8 when used without or with a small number of plugins, alongside Black, and on Python 3 code. It implements every rule in Flake8 under those conditions and re-implements some popular Flake8 plugins natively. Additionally, Ruff has a larger rule set, implementing over 800 rules compared to Flake8's ~409 total rules. However, Ruff does not support custom lint rules and does not include all the 'opinionated' rules from flake8-bugbear.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route(\"Why use ruff over flake8?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bbd4206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The themes about time that appear in the sonnets include the inevitability of decay and the passage of time leading to loss. Time is depicted as a force that causes beauty and youth to fade, as seen in the imagery of nature's cycles, such as the day turning to night and plants that hold perfection only for a moment. The sonnets reflect on how everything grows and then diminishes, emphasizing the transient nature of life. Additionally, there is a sense of urgency to preserve beauty and love against the relentless advance of time, suggesting that procreation or creating a legacy is a way to defy time's scythe. Overall, time is portrayed as both a destructive force and a motivator for the speaker to immortalize beauty through verse.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route(\"What themes about time appear in the sonnets?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d87282c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route(\"What did biden say about ketanji brown jackson in the state of the union address?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a0cbbe",
   "metadata": {},
   "source": [
    "## Multi-Hop vector store reasoning\n",
    "\n",
    "Because vector stores are easily usable as tools in agents, it is easy to use answer multi-hop questions that depend on vector stores using the existing agent framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d397a233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "@tool\n",
    "def sonnets_qa(question: str) -> str:\n",
    "    \"\"\"Answer questions about Shakespeare's sonnets. Use for literary themes, lines, or context from the sonnets.\"\"\"\n",
    "    return sonnets_chain.invoke(question)\n",
    "\n",
    "@tool\n",
    "def ruff_qa(question: str) -> str:\n",
    "    \"\"\"Answer questions about Ruff, the Python linter. Use for configuration, features, and behavior from the Ruff FAQ.\"\"\"\n",
    "    return ruff_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06157240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multi_hop_agent = create_agent(model, tools=[sonnets_qa, ruff_qa])\n",
    "\n",
    "def ask_multi(q: str):\n",
    "    res = multi_hop_agent.invoke({\"messages\": [HumanMessage(q)]})\n",
    "    return res[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a222a077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ruff uses **nbQA** to run over Jupyter Notebooks. You can run Ruff over a notebook using the command: `$ nbqa ruff Untitled.ipynb`.\\n\\nAs for Shakespeare's sonnets, they do not mention anything related to tools or notebooks.\\n\\nRegarding whether the president mentioned Ruff in the state of the union, I don't have that information.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_multi(\n",
    "    \"What tool does Ruff use to run over Jupyter Notebooks? \"\n",
    "    \"Do the sonnets mention anything related to that tool or notebooks? \"\n",
    "    \"Did the president mention that tool in the state of the union?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-env-311)",
   "language": "python",
   "name": "ml-env-311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
